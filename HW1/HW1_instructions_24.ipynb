{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW1: Data exploration and Dimensionality reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment you will explore the dataset, handle the missing values, standardize the numerical values and reduce the dimensionality of the feature space. The learning outcome of this part is to know how one can pre-process a real-world dataset and prepare for an supervised or unsupervised learning task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Student information\n",
    "Please provide your information for automatic grading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STUD_SUID = 'Your su account (e.g., pang1234)'\n",
    "STUD_NAME = 'First and Last name'\n",
    "STUD_EMAIL = 'student@stud.dsv.su.se'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Grading: \n",
    "\n",
    "Pass/Fail"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OUTLINE: \n",
    "\n",
    "Data pre-processing, plotting and dimensionality reduction\n",
    "\n",
    "1. Reading the file\n",
    "2. Missing Values\n",
    "3. Impute with scikit-learn\n",
    "4. Implement imputation\n",
    "5. Plotting\n",
    "6. Standardization\n",
    "7. Dimensionality reduction\n",
    "8. Multi-Dimensional Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NOTE:\n",
    "\n",
    "Each function you make will be considered during the grading, so it is important to strictly follow input and output instructions stated in the skeleton code.\n",
    "\n",
    "You should not delete any of the given cells or change the structure of the cells or add cells (unless completely necessary, add a comment on why you added a cell) as they will help in grading the assignment.\n",
    "\n",
    "Some variable names are already given and have random values or empty arrays assigned on them. In this case you should only change the assignments on the variables but keep the names as given.\n",
    "\n",
    "When you are finished with implementing all the tasks, **clear all outputs, run all cells again** (make sure there is no error) and submit!\n",
    "\n",
    "Make sure that the results and figures asked are visible for us to grade. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# these are the libraries that you will need throughout the assignment\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import MDS\n",
    "\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "\n",
    "RSEED = 8\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DATA PRE-PROCESSING, PLOTTING AND DIMENSIONALITY REDUCTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use **Pima Indians Diabetes Database** that is publicly available and from UCI. However, we removed and changed some parts of the dataset for the homework evaluation, so **please use the one in the zip file, not the original one**.\n",
    "\n",
    "This dataset is originally from the National Institute of Diabetes and Digestive and Kidney Diseases. The objective of the dataset is to diagnostically predict whether or not a patient has diabetes, based on specific diagnostic measurements included in the dataset. Several constraints were placed on the selection of these instances from a larger database. In particular, all patients here are females at least 21 years old of Pima Indian heritage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset consists of several medical predictors (independent) variables and one target (dependent) variable, Outcome. Independent variables include the number of pregnancies the patient has had, their BMI, insulin level, age, and so on.\n",
    "\n",
    "According to the information on the data, it has eight attributes and one binary class. The brief explanation of the attributes are as follows:\n",
    "\n",
    "- Pregnancies: Number of times pregnant.\n",
    "\n",
    "- Glucose: Plasma glucose concentration a 2 hours in an oral glucose tolerance test.\n",
    "\n",
    "- BloodPressure: Diastolic blood pressure (mm Hg).\n",
    "\n",
    "- SkinThickness: Triceps skin fold thickness (mm).\n",
    "\n",
    "- Insulin: 2-Hour serum insulin (mu U/ml).\n",
    "\n",
    "- BMI: Body mass index (weight in kg/(height in m)^2).\n",
    "\n",
    "- DiabetesPedigreeFunction: Diabetes pedigree function.\n",
    "\n",
    "- Age: Age (years).\n",
    "\n",
    "- and we have a binary class which can be 0 (healthy) or 1 (diabetes)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *1.* Reading the file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Task: Read the dataset using pandas. Use the csv file called diabetes.csv that you will find on ilearn.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here\n",
    "# call your dataset: data\n",
    "\n",
    "data = pd.DataFrame() # change this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do not delete this!\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# if you want to see information about the dataset:\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# if you want to see information about the dataset, uncomment:\n",
    "# data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *2.* Missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### It seems like there is no null data encoded as such. However, if you check the values in the dataset, there are many that might look strange (maybe some values that do not make sense?). \n",
    "\n",
    "### `Task: Plot a bar plot of the 'missing' values (values that are encoded with values that do not make sense) per attribute, excluding the attributes 'Pregnancies', 'Outcome'. The plot must have a title and the bars of the plot must be named to their respective attribute names.` \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the steps are just indicative, if you want to do it your own way, please do so as long as you print the required barplot, specified above \n",
    "# in the task description. \n",
    "# The style, color, or library used for plotting will not affect the grading of the task (or any of the tasks that require plots)\n",
    "\n",
    "\n",
    "# step 1: store the sum of missing values per attribure (excluding the attributes 'Pregnancies', 'Outcome') in a pandas Series\n",
    "# step 2: plot the missing values series as a barplot, using the plot function from pandas (or any other library, as you prefer). \n",
    "\n",
    "# Write your code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *3.* Impute with scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Is it normal that people's BMI is so low, in some cases? so low that they seem to not have a mass? or not? What about Glucose, and Blood Pressure? You may want to change some values into another reasonable value, such as mean or median. The only thing that can have zero value is the attribute **'Pregnancies'**. \n",
    "\n",
    "### **NOTE**! The attribute **'Outcome'** is your class. The class SHOULD NOT be included the cleaning process.\n",
    "\n",
    "### `Task: Impute the missing values using the SimpleImputer from scikit-learn with strategy = 'mean'. Return a pandas dataframe called 'data_imputed' that includes both the imputed attributes and the ones that you did not impute (Pregnancies, Outcome). The dataframe called 'data' SHOULD REMAIN UNCHANGED (not imputed) as it will be used again in the next task.`\n",
    "### For the scikit-learn imputation, you can find more information [here](https://scikit-learn.org/stable/modules/impute.html), or in the recorded lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scickit-learn version\n",
    "# The steps are just indicative, if you want to do it your own way, please do so as long you store in data_imputed a dataset that includes \n",
    "# both the imputed attributes and the ones that you did not impute (Pregnancies, Outcome). The dataframe 'data' must remain unchanged (not imputed). \n",
    "  \n",
    "# step 1: create a copy of the original dataset called data_imputed\n",
    "# step 2: create a list with the names of the attributes that you will impute, call it columns_to_impute\n",
    "# step 3: create a new dataset called df_part, that includes only the columns that you will impute (the ones from step 2)\n",
    "# step 4: define the SimpleImputer object (from sklearn) with strategy='mean' and call it imputer, fit and transform the dataset that you created in step 3 \n",
    "# step 5: convert the resulting array from step 4 into a dataframe, as column names you can pass the names of attributes of the list that you created in step 2. call it df_converted\n",
    "# step 6: in data_imputed (which is still a copy of data (the original dataset) replace the attributes you wanted to impute with the attribures in df_converted. The dataframe called 'data' should remain \n",
    "# unchanged. \n",
    "# data_imputed should contain all 8 attributes and the class, where every attribute -except 'Pregnancies' and 'Outcome'- have imputed values \n",
    " \n",
    "\n",
    "\n",
    "# Write your code here\n",
    "\n",
    "\n",
    "data_imputed = pd.DataFrame([]) #change this\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you successfuly finished the imputation for task3, uncomment the following:\n",
    "#imputer.statistics_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you successfuly finished the imputation for task3, uncomment the following:\n",
    "#data_imputed.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you successfuly finished the imputation for task3, uncomment the following:\n",
    "#data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *4.* Implement imputation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Task: Write a function that changes the zero (or nan) value to the mean of the attribute of a dataframe. `\n",
    "\n",
    "### You will impute the missing values with the mean of the column without using scikit-learn. You will store the resulting dataset in diabetes_1, that includes both the imputed attributes and the ones that you did not impute (Pregnancies, Outcome). diabetes_1 will not be used again after this task. \n",
    "\n",
    "Note: **Do not** use sklearn here. The goal is for you to write your *own* imputation function, and to use it. Since you **will not** use sklearn, there is no need to convert the imputed dataframe to a dataframe again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imputation(df, columns_to_imputed):\n",
    "    \"\"\"\n",
    "     A function to change nan value (or zero value) to the mean of the attribute\n",
    "        \n",
    "        # the steps are just indicative, implement it the way you like as long as you return a dataframe \n",
    "        # that includes both the imputed attributes and the ones that you did not impute. \n",
    "\n",
    "        - Step 1: Get a part of dataframe using columns received as a parameter.\n",
    "        - Step 2: Change the zero values in the columns to np.nan\n",
    "        - Step 3: Change the nan values to the mean of each attribute (column). \n",
    "                  You can use the apply(), fillna() functions.\n",
    "        \n",
    "        Input:\n",
    "          df: A dataframe to which we will apply imputation\n",
    "          columns_to_imputed: A list of columns that need to be imputed\n",
    "          \n",
    "        Output:\n",
    "          An imputed dataframe\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    #Write your code here\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    return None #change this None to the df to be returned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\"Glucose\", \"BloodPressure\", \"SkinThickness\", \"Insulin\", \"BMI\", \"DiabetesPedigreeFunction\", \"Age\"]\n",
    "diabetes_1 = imputation(data, columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you successfuly finished the imputation, uncomment this\n",
    "\"\"\"\n",
    "try:\n",
    "    np.testing.assert_allclose(data_imputed.values, diabetes_1.values)\n",
    "    print(\"result: equal\")\n",
    "except:\n",
    "    print(\"result: not equal\")\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you successfuly finished the imputation, uncomment this\n",
    "#diabetes_1.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you successfuly finished the imputation, uncomment this\n",
    "#data_imputed.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *5.* Plotting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a. In this task we will explore the attribute 'Glucose' with plotting. \n",
    "\n",
    "### `Task: Create 1 figure with a set of 3 subplots. We have seen something very similar in the lab, with 4 subplots. `\n",
    "\n",
    "\n",
    "#### - `In the 1st subplot create a boxplot for the attribute Glucose.`\n",
    "\n",
    "\n",
    "#### - `In the 2nd subplot create a histogram for the attribute Glucose.`\n",
    "\n",
    "\n",
    "#### - `In the 3rd subplot create a scatterplot for the attributes Glucose and BloodPressure, colored by the attribute 'Outcome'(add a legend to identify color - class label pairs).`\n",
    "`The plots should have titles, the figsize should be big enough (for example figsize=(16,9)). If you cannot create any of the required plots either remove the respective axis and make for example  1 figure with 2 subplots or create the figure with three subplots and leave the respective axis empty.'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 1 figure with a set of 3 subplots. Each axes should contain a figure as described below: \n",
    "# Figure 1: A boxplot for the attribure 'Glucose'\n",
    "# Figure 2: A histogram for the attribure 'Glucose'\n",
    "# Figure 3: A scatterplot of Glucose vs BloodPressure, colored by the attribure 'Outcome', include a legend to identify which class-color pairs\n",
    "\n",
    "# All subplots must have a title that makes sense.\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(1, 3, figsize=(16,9)) #create 1 row with 3 plots\n",
    "#Write your code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b.\tAge by class \n",
    "\n",
    "### `Task:  Plot the 'Age' attribute in groups of 10 years in relation to the class ('Outcome').`\n",
    "\n",
    "Information about the cut function [here](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.cut.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 1: Divide the age column into the following age groups: 0-19, 20-29, 30-39, 40-49, 50-59, 60-69, 70-79, 80-100 using the cut function from pandas. \n",
    "# The new column should be called age_bins.\n",
    "# step 2: Store in age_by_class, a dataframe with the counts of unique values of the attribute age_bins by class. You can use groupby() and value_counts()\n",
    "# step 2: Plot a barplot for the age_by_class. You can use pandas to plot it.\n",
    "# step 3: **DROP THE age_bins** attribute from data_imputed after you have plotted the barchart. \n",
    "# do not skip dropping the age_bins!\n",
    "\n",
    "# Write your code here\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do not delete this\n",
    "data_imputed.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## c.\tYour own multivariate question\n",
    "\n",
    "### `Task: think of a multivariate question that can be answered with a plot. Write it in the markdown cell below this. Write the code to create the plot in the code cell below. Write a short answer to your question (max 50 words) in the markdown cell after that.`\n",
    "\n",
    "Bonus points (only in how much I'll enjoy the answer, since there are no points in this homework) if your question is original, so please don't make me read 300 copies of the same question :D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write your multivariate question here. Delete this text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here. Remember to restore the dataframe to its original state after you have plotted your plot, or create a copy to use for this exercise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write your short answer (max 50 words) here. Delete this text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *6.* Standardization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardization transforms data to have a mean of zero and a standard deviation of 1. \n",
    "\n",
    "### It is a crucial step before performing PCA, since we are interested in the components that maximize the variance. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### `Task: Standardize the data_imputed dataset. You can use sklearn.  Store in a variable called 'y' the attribute 'Outcome' (your class)`\n",
    "### NOTE! Outcome is the class of the dataset indicating if a patient is healthy or has diabetes. As we discussed in the lab, the class should not be included in the standardization. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 1: Use StandardScaler to fit_transform data_imputed, excluding the class (Outcome)\n",
    "# step 3: Transform the standardized numpy matrix returned by StandardScaler into a dataframe called data_standardized.\n",
    "# step 4: Rename the columns of the dataframe with their corresponding names.\n",
    "# step 5: Store in a variable called y the attribute Outcome (your class), do not skip this\n",
    "\n",
    "# Write your code here\n",
    "\n",
    "y = pd.Series() # change this\n",
    "data_standardized = pd.DataFrame() # change this \n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do not delete this\n",
    "data_standardized.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do not delete this\n",
    "y.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *7.* Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a. PCA\n",
    "\n",
    "### `Task: Reduce the dimensionality of the standardized dataset in 2 Principal Components, with Principal Component Analysis. Print the information obtained by the following attributes of the pca object: explained_variance_ratio and components_. `"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step1: Reduce the dimensionality of the standardized dataset to 2 Principal Components, with Principal Component analysis. You can use PCA from sklearn. Use random state = 8\n",
    "# step2: Store the explained variance ratio in an array called explained_variance_ratio.\n",
    "# step3: Store in a dataframe called df_principal_components, the result of pca's attribute components_,\n",
    "# (Principal axes in feature space, representing the directions of maximum variance in the data), with the respective attribute names.\n",
    "\n",
    "# Write your code here\n",
    "\n",
    "explained_variance_ratio =  None #change this\n",
    "df_principal_components = pd.DataFrame() #change this\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_principal_components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "explained_variance_ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b. \tWhich attribute contributes the most?\n",
    "\n",
    "### `Task: Store in a variable called attribure_contributing_the_most, which attribure contributes most to the variance of the 1st PC.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Which attribute contributes the most in the variance of the 1st principal component? \n",
    "# store the name of the attribute here, as a string type:\n",
    "\n",
    "\n",
    "attribute_contributing_the_most =  \" ? \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do not delete this\n",
    "attribute_contributing_the_most"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.  Multi-Dimensional Scaling\n",
    "\n",
    "Multidimensional scaling (MDS) seeks a low-dimensional representation of the data in which the distances respect well the distances in the original high-dimensional space.\n",
    "In general, MDS is a technique used for analyzing similarity or dissimilarity data and it can help visualize the distances or dissimilarities between sets of objects. Examples of similarity or dissimilarity data might include the distance between pairs of cities, or planets at a particular point in time, or the similarity among groups of people (voters, patients etc). \n",
    "\n",
    "In these last two excercises we will apply Multi-Dimensional Scaling in our patient dataset using two different versions of the MDS sklearn algorithm. \n",
    "\n",
    "We will focus on the attribute **dissimilarity** of the MDS object. The attribute can be either 'euclidean' or 'precomputed'. In the former case the euclidean distance between the data points is computed by the algorithm, while in the latter case the user must themeselves compute the dissimilarities between data points and pass this to fit_transform.  \n",
    "\n",
    "Please check the sklearn page for MDS to be able to implement the above tasks: [here](https://scikit-learn.org/stable/modules/generated/sklearn.manifold.MDS.html). Make sure you understand the parameters of fit_transform and how you could use them for the tasks!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a. Multi-Dimensional Scaling, task a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###   `Task: Apply MDS on the data_standardized with n_components=2 and dissimilarity='euclidean'. Plot the 2 resulting coordinates (in a scatterplot) with colors respective to the class labels.`\n",
    "\n",
    "Note: This is very similar to how we applied sklearn's PCA on task 7!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 1: Initialize MDS with n_components = 2, random_state=8.\n",
    "# step 2: Fit and transform the standardized dataset\n",
    "# step 3: Plot the resulting reduced dataset with colors respective to the class (Outcome).\n",
    "\n",
    "# write your code here\n",
    "\n",
    "X_2d_a = np.zeros([2, 2]) # change this assignment on X_2d_a, but keep X_2d_a as the name of the resulting MDS numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_2d_a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End of assignment 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `NOTE: Each function you make will be graded, so it is important to strictly follow input and output instructions stated in the skeleton code.  You should not delete any of the given cells or change the structure of the cells or add cells (unless completely necessary, add a comment on why you added a cell) as they will help in grading the assignment. Some variable names are already given and have random values or empty arrays assigned to them. In this case you should only change the assignments on the variables but keep the names as given. When you are finished with implementing all the tasks, clear all outputs, run all cells again (make sure there is no error) and submit! Make sure that the results and figures asked are visible for the grading. ` \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b. Multi-Dimensional Scaling, task b (not graded, not part of the assignment, you can avoid doing it if you don't want to).\n",
    "\n",
    "There are no bonus points for this exercise. You can see it as a personal challenge if you want to do more than the basic and test your knowledge, it will not be graded (but I will go over it if you attempt it).\n",
    "\n",
    "Don't let it distract you from the rest of the lab, which you actually need to pass. You have nothing to gain from this exercise, and nothing to lose (apart from the time that you'll use to complete the exercise).\n",
    "\n",
    "Feel free to submit an incomplete solution if you want to try, but still don't manage to get a complete one, and you will get feedback on the missing steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Task: Compute the pairwise distances between observations using the euclidean metric. Apply MDS on the custom similarity matrix with n_components=2. Plot the results with colors respective to the class label. `\n",
    "\n",
    "Note: the resulting plot should look similar with the one of Task 8a. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 1: compute the pairwise distances between observations using the euclidean metric. \n",
    "\n",
    "# One of the ways to do this is to use pdist and squareform from the scipy library (see [here](https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.distance.pdist.html)) \n",
    "# or euclidean_distances from sklearn. In any case the returned array should be: distances ndarray of shape (n_samples_data_standardized, n_samples_data_standardized)\n",
    "\n",
    "# step 2: create an MDS object with n_components=2, random_state=8. What should the dissimilarity parameter be in this case?\n",
    "# step 3: apply MDS on the constructed square distance matrix from step 1\n",
    "# step 4: plot the results in a scatterplot with colors respective to the class label\n",
    "\n",
    "\n",
    "\n",
    "# write your code here\n",
    "\n",
    "X_2d_b = np.zeros([2, 2]) # change this assignment on X_2d_b, but keep X_2d_b as the name of the resulting MDS numpy array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do not delete this \n",
    "try:\n",
    "    np.testing.assert_allclose(X_2d_b, X_2d_a, atol=0.00001)\n",
    "    print(\"result: equal\")\n",
    "except:\n",
    "    print(\"result: not equal\")\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d419f0c7497595e68d237e2dc8f4eac5bf2e06b53acfc5f80d3651d7a9e90a8a"
  },
  "kernelspec": {
   "display_name": "Python 3.10.0 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
