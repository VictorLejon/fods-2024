{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 1: EDA, Pre-processing, and Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Outline:**\n",
    "\n",
    "1. Reading the file, Summary Statistics\n",
    "\n",
    "2. Missing Values and Imputation\n",
    "\n",
    "3. EDA (Plotting, Outliers, Highly correlated features)\n",
    "\n",
    "4. Standardization\n",
    "\n",
    "5. Principal Component Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Import libraries\n",
    "\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "\n",
    "#download scikit-learn\n",
    "#We will use scikit-learn library during this course. Scikit-learn is an open source machine learning library that supports supervised and unsupervised learning \n",
    "#pip install scikit-learn \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can check the version using __version__\n",
    "\n",
    "pd.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Reading the file and summary statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading the file\n",
    "\n",
    "\n",
    "In this session we will use the cleveland, heart disease dataset.  \n",
    "[link]( https://archive.ics.uci.edu/ml/datasets/heart+disease) to the dataset with information. \n",
    "\n",
    "#Note:\n",
    "The header parameter specifys the Row number(s) to use as the column names, and the start of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Note:\n",
    "# The header attribute specifys the row number(s) to use as the column names, and the start of the data. \n",
    "\n",
    "data = pd.read_csv(\"processed.cleveland.data\", header=None)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Dataset Information as found in heart-disease.names**\n",
    "\n",
    "This database contains 14 attributes and 303 patients that might not or might have a heart disease diagnoses. \n",
    "\n",
    "There are both continuous and categorical attributes. \n",
    "\n",
    "For example, the dataset has the age of the patients, resting blood pressure, maximum heart rate for numerical features. \n",
    "While sex, chest pain type or number of major vessels are categorical features. \n",
    "\n",
    "The class label is categorical, consists of labels from 0 to 4 and refers to a diagnosis of a heart disease. \n",
    "Experiments with the Cleveland database have concentrated on simply attempting to distinguish presence (values 1,2,3,4) from absence (value 0).\n",
    "\n",
    "\n",
    "\n",
    "### Feature explanation\n",
    "- age = in years // numerical, continuous\n",
    "\n",
    "- sex    =   (0 is female, 1 is male) // categorical\n",
    "\n",
    "\n",
    "- cp     = chest pain type (1 -> typical angina,  2 -> atypical angina,  3 -> non-anginal, 4 -> asymptomatic) //categorical\n",
    "\n",
    "- trestbps = resting blood pressure//continuous\n",
    "\n",
    "- chol      = serum cholestral in mg/dl //continuous\n",
    "\n",
    "- fbs       = fasting blood sugar > 120 mg/dl is 1 otherwise 0 //categorical\n",
    " \n",
    "- restecg   = resting electrocardiographic result, 0 -> normal, 1 -> St-T wave abnormality, 2 -> probable or definite hypertropy//categorical\n",
    "\n",
    "- thalach   = maximum heart rate achieved//continuous\n",
    "\n",
    "- exang     = exercise induced angina (1 = yes, 0 = no)//categorical\n",
    "\n",
    "- oldpeak   = ST depression induced by exercise relative to rest//continuous\n",
    "\n",
    "- slope     = the slope of the peak exercise ST segment (1 -> upslopping, 2 -> flat, 3 -> downslopping)//categorical\n",
    "\n",
    "- ca        = number of major vessels (0-3) covered by flourosopy//categorical\n",
    "\n",
    "- thal      = (3 -> normal, 6 -> fixed defect, 7 -> reversible defect)//categorical\n",
    "\n",
    "- class     = diagnosis of heart disease//categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Change the column/feature names\n",
    "\n",
    "The name of the columns and generally information about this dataset can be found under the heart-disease.names file. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set_axis assigns a  desired index to given axis. \n",
    "#Input:\n",
    "#    a list-like index\n",
    "#    the axis to update\n",
    "\n",
    "data = data.set_axis(['age', 'sex', 'cp', 'trestbps', 'chol', 'fbs', 'restecg', 'thalach', 'exang', 'oldpeak', 'slope', 'ca', 'thal', 'class'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To summarize: \n",
    "\n",
    "- numerical are: age, trestbps, chol, thalac, oldpeak\n",
    "- categorical are: sex, cp, fbs, restecg, exang, slope, thal, class\n",
    "- our class/target is the column class\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Information and summary statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can see the first rows of the dataset by calling the head() method and inside there pass a small number of rows\n",
    "data.head(10)\n",
    "\n",
    "# Try to see what changes if you pass a different number of rows!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check number of rows(patients) and colums(features)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can print information about the dataset by using the info() method. \n",
    "\n",
    "We can see that the attributes ca and thal are of object type.\n",
    "\n",
    "A data type is essentially an internal construct that a programming language uses to understand how to store and manipulate data. \n",
    "- int: for integer numbers\n",
    "- float: for floating point numbers\n",
    "- object: for text or mixed numeric and non-numeric values\n",
    "\n",
    "Data types are one of those things that you donâ€™t tend to care about until you get an error or some unexpected results. \n",
    "It is also one of the first things you should check once you load a new data into pandas for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the data types of the dataset\n",
    "print('Data Show Info\\n')\n",
    "data.info()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can print some summary statistics about the data using the describe() method.\n",
    "\n",
    "It can be useful to spot columns containing outliers, or erroneous data: what if the max value of age was 1234 years? or if the minimum value was -3 years?\n",
    "\n",
    "It is always a good idea to take a look at the description of your data before proceeding any further, to see if anything does look out of place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Data Show Describe\\n')\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally the columns names can be accessed with the columns attribute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that, while methods are called with the syntax _method()_, attributes are called using just _attribute_.\n",
    "\n",
    "If you try to call an attribute using the method syntax, you will get a TypeError, as in the next cell. You can get familiar with the error deleting the # at the beginning of the line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data.columns()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Outline:**\n",
    "\n",
    "1. Reading the file, Summary Statistics âœ…\n",
    "\n",
    "2. Missing Values and Imputation ðŸ”œ\n",
    "\n",
    "3. EDA (Plotting, Outliers, Highly correlated features)\n",
    "\n",
    "4. Standardization\n",
    "\n",
    "5. Principal Component Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Missing Values and Imputation\n",
    "\n",
    "Missing values: when no data are stored (due to some failure to record them or maybe data corruption).\n",
    "We need to handle them, as many machine learning algorithms do not support missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Getting Started](missing_data.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes missing values will be marked as such, but also as some random string like '?', '_', essentially when no data are stored. \n",
    "\n",
    "\n",
    "isnull() will return in every cell of the df True if the value is Null or False if it's not. \n",
    "if on that you additionally call any(),  it will summarize the results along a column "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking for null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking for null values\n",
    "# To detect NaN values pandas uses either .isna() or .isnull()\n",
    "\n",
    "data.isnull().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking for missing values in other forms\n",
    "\n",
    "The previous command tells us that there are no missing values, but it only looks for None and NaN values. \n",
    "\n",
    "We have noticed that that the attributes \"ca\" and \"thal\" are of object type which means they may have mixed types. \n",
    "Let's see what is unique about them compared to the others!\n",
    "\n",
    "Rule of thumb! ALWAYS open the dataset file and investigate the data yourself and look for inconsistencies. \n",
    "Missing values might be written like: \"missing\", \"?\", \"_\", or others, and if they are located in a column that contains strings it might not be evident that they are missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the unique values in the \"ca\" and \"thal\" columns\n",
    "\n",
    "print(data['ca'].unique())\n",
    "print(data['thal'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'?' doesn't look like a proper value! Someone might have used it instead of putting a proper missing value flag."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to handle the missing values\n",
    "\n",
    "In general,  there are different ways of dealing with missing values\n",
    "\n",
    "For example:\n",
    "\n",
    " \n",
    "1. if the feature is nominal (categorical), replace missing with most frequent category\n",
    "2. if feature is numeric, replace it with the mean value\n",
    "3. simply delete the rows that have missing values if there is an insignificant amount of them\n",
    "4. delete the columns that have missing values if they are the majority of the values in that column\n",
    "5. advanced approaches, like using ML to predict the missing values, are also possible\n",
    "\n",
    "The simplest approach for dealing with missing values is to remove entire predictor(s) and/or sample(s) that contain missing values, but it might bring to a significant loss of data.\n",
    "We need to check how many missing values are in each column before deleting everything."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Small example:\n",
    "Now we have a table with missing values. Take a look at it: how should we handle the missing values in this case?\n",
    "\n",
    "![Small table](small_table.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How many missing values per feature?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ca: \", (data['ca'] == \"?\").sum())\n",
    "print(\"thal: \", (data['thal'] == \"?\").sum())\n",
    "print(\"#################\")\n",
    "print(\"whole df/ per column:\\n\", (data == \"?\").sum(axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete rows with missing values\n",
    "\n",
    "drop every row that contains '?' in the two columns that we have spotted.\n",
    "\n",
    "In case you have empty values in the form of nan, you can use a specific Pandas method, _dropna()_, with\n",
    "dataframe = dataframe.dropna()\n",
    "\n",
    "We will use the version of the dataset without missing values for the rest of the lab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dropped = data[(data[\"ca\"] != '?') & (data[\"thal\"] != '?')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dropped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imputation using the most frequent value with sklearn\n",
    "\n",
    "The missing values are just a few so we can simply delete the rows that contain missing values. For the rest of the assignment we will use the dataframe with the dropped missing values.  \n",
    "However, this is an example of how you can impute missing values using the SimpleImputer from sklearn. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#download sklearn first if you haven't already!\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "#first we define the object\n",
    "imputer = SimpleImputer(missing_values=\"?\", strategy='most_frequent')#it works along the columns\n",
    "#then fit it to the data\n",
    "imputer.fit(data)\n",
    "\n",
    "data_imputed = imputer.transform(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SimpleImputer allows us to choose different strategies.\n",
    "In this case, since the data was categorical, choosing the average or median would have been incorrect, and the mode was the appropriate choice.\n",
    "\n",
    "If the data to be imputed contained a numeric variable, we could have chosen a different strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert it to a dataframe\n",
    "data_imputed = pd.DataFrame(data_imputed, columns=data.columns)\n",
    "data_imputed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer.statistics_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Outline:**\n",
    "\n",
    "1. Reading the file, Summary Statistics âœ…\n",
    "\n",
    "2. Missing Values and Imputation âœ…\n",
    "\n",
    "3. EDA (Plotting, Outliers, Highly correlated features) ðŸ”œ\n",
    "\n",
    "4. Standardization\n",
    "\n",
    "5. Principal Component Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Exploratory Data Analysis (EDA)\n",
    "Discover patterns, spot anomalies, check assumptions with the help of summary statistics and  graphical representations. \n",
    "\n",
    "\n",
    "Two types of EDA:\n",
    "- Univariate: data being analyzed consists of just one variable. You can use Histograms, Barplots, Boxplots (can be used for outliers) etc\n",
    "\n",
    "- Multivariate : investigate the relationship between two or more variables of the data through cross-tabulation or statistics.  Grouped bar plots, scatterplots, heatmaps for correlation etc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How many patients are diagnosed with a heart disease? \n",
    "\n",
    "**Remember**:\n",
    "- presence of heart disease: values 1,2,3,4 \n",
    "- absence: value 0\n",
    "\n",
    "The method *value_counts()* returns a series containing the count for each unique value in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#count_values(): returns a series containing counts of unique values\n",
    "\n",
    "data_dropped[\"class\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Balancing the problem\n",
    "\n",
    "There are 160 patients who do not have heart disease.\n",
    "While the patients who are positive to heart disease are distributed among labels 1 to 4, with each class having a small number of patients.\n",
    "We can make the problem more balanced by transforming all these positive class labels to 1.\n",
    "\n",
    "This process can also be used later on in supervised learning to make your dataset more balanced, but for now we will do it for demonstration and visualization purposes.\n",
    "\n",
    "We will use the _replace()_ method to map the values  2, 3, 4 to 1. Another possibility would be to use the numpy method _np.where()_ to change the values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we are replacing 2,3,4 with 1. The loc method is used to access a group of rows (all of them, since we are using :) and columns (only column 'class', in this case).\n",
    "data_dropped['class'] = data_dropped.loc[:, 'class'].replace({2: 1, 3: 1, 4:1})\n",
    "\n",
    "# alternative: data_dropped['class'] = np.where((data_dropped['class']>0),1,0).\n",
    "# in this case, we would be taking only a slice of our dataframe (indicated by the label 'class') and replacing the values bigger than 0 with 1 and the rest with 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's count the values again\n",
    "data_dropped[\"class\"].value_counts()\n",
    "\n",
    "# We have a more balanced dataset now!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dropped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the class - barplot\n",
    "\n",
    "The figure object is your empty canvas.\n",
    "\n",
    "We will use _plt.style.use('ggplot')_: the  \"ggplot\" style adjusts the style of our plot to emulate ggplot (a popular plotting package for R). Feel free to experiment changing the styles.\n",
    "\n",
    "Barplot parameters: \n",
    "    x : a sequence of scalars (unique values of the class label)\n",
    "    y the height of the bars (how many samples each class has)\n",
    "\n",
    "set_xticks(): Set the x ticks with list of ticks\n",
    "    Essentialy here we will pass the names of the bars. \n",
    "\n",
    "grid(True): Whether to show the grid lines. True/False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first create a series where you will store the class label\n",
    "class_label = data_dropped[\"class\"]\n",
    "# replace the 0 with negative and 1 with positive, for the sake of interpretation/visualization\n",
    "class_label = class_label.replace({0: 'negative', 1: 'positive'}) # Note that we are using the same method as before!\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot()\n",
    "\n",
    "ax.bar(class_label.unique(), class_label.value_counts())\n",
    "ax.set_title(\"Class/Target\")\n",
    "ax.set_xticks(class_label.unique())\n",
    "ax.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for outliers with boxplots\n",
    "\n",
    "Outliers can either be a mistake or just variance.\n",
    "\n",
    "To check for outliers we can use the boxplot to see the distribution of the attributes. \n",
    "Any outliers are normally outside the plot region.\n",
    "\n",
    "We will create boxplots for all the numerical features\n",
    "\n",
    "Instead of plt.figure we can call plt.subplots and specify how many rows and columns of plots we want.\n",
    "if nrows=1, ncols=2 it will create 1 row with 2 plots/ if nrows=2, ncols=3 it will create 2 rows with 3 plots each.\n",
    "\n",
    "Note that the axes attribute is just a list of the matplotlib axes, so we can actually iterate through them and create different plots!\n",
    "\n",
    "The *tight_layout()* method automatically adjusts subplot params so that the subplot(s) fits in to the figure area\n",
    "\n",
    "\n",
    "barplot:\n",
    "    x: The input data.\n",
    "\n",
    "There are a few variants of boxplots out there, but the one that we will use will show\n",
    "- the median value with a coloured line\n",
    "- the box, extending from the first to the third quartile\n",
    "- the whiskers, extending from the first (or the third) quartile to include the data points that are at most as distant as 1.5 times the interquartile range from it\n",
    "- the points after the whiskers, indicating the outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 4)\n",
    "\n",
    "plt.tight_layout()\n",
    "ax[0].set_title(\"trestbps\")\n",
    "ax[0].boxplot(data_dropped['trestbps'])\n",
    "\n",
    "ax[1].set_title('chol')\n",
    "ax[1].boxplot(data[\"chol\"])\n",
    "\n",
    "ax[2].set_title('thalach')\n",
    "ax[2].boxplot(data[\"thalach\"])\n",
    "\n",
    "ax[3].set_title('oldpeak')\n",
    "ax[3].boxplot(data[\"oldpeak\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grouped bar-plot\n",
    "\n",
    "If we want to compare the values of males and females for each of the class labels, we can make a grouped box-plot.\n",
    "\n",
    "_groupby()_: allows us to group rows together based on a column and perform an aggregate function on them. \n",
    "After _groupby()_, we need to specify a summarization function, such as *value_counts()*, _sum()_, or _average()_.\n",
    "\n",
    "To create the plot, we call unstack, which pivots the grouped dataframe back, and just call plot with kind='bar'\n",
    "\n",
    "If we use True for the stacked option, the bars for the different class labels will be put one top of each other, instead of next to each other. \n",
    "Converting it to False we can see the difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ylabels = ['negative', 'positive']\n",
    "labels = (\"female\", \"male\")\n",
    "\n",
    "positions = (0, 1)\n",
    "\n",
    "s_x = data_dropped.groupby(\"sex\")['class'].value_counts()\n",
    "s_x.unstack().plot(kind='bar', stacked=True)\n",
    "\n",
    "plt.legend(labels=ylabels)\n",
    "plt.xticks(positions, labels)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check how the _groupby()_ and _unstack()_ methods work:\n",
    "\n",
    "s_x contains the 'class' column of the data grouped by sex, using value counts as summarization function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if we call unstack the dataframe now is pivoted back!\n",
    "s_x.unstack()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grouped bar-plot\n",
    "How many people are negative to heart disease and how many are posive, per age value?\n",
    "\n",
    "We can basically repeat the same procedure, but grouping by age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "by_age= data_dropped.groupby([\"age\"])['class'].value_counts()\n",
    "by_age.unstack().plot(kind='bar', stacked=True)\n",
    "plt.legend(labels=ylabels)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#similarly\n",
    "fbs_by_class = data_dropped.groupby(\"fbs\")['class'].value_counts()\n",
    "fbs_by_class.unstack().plot(kind='bar', stacked= False)\n",
    "plt.legend(labels=ylabels)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boxplot with seaborn \n",
    "Age in relation to class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#boxplot of age in relation to the class\n",
    "\n",
    "f, ax = plt.subplots(figsize=(8, 6))\n",
    "sns.boxplot(x=\"class\", y=\"age\", data=data_dropped)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Highly correlated features and Heatmap\n",
    "\n",
    "A strong correlation is indicated by a Pearson Correlation Coefficient value near 1.\n",
    "Pearson's correlation coefficient is the covariance of the two variables divided by the product of their standard deviations. \n",
    "\n",
    "Therefore, when looking at the Heatmap, we want to see what correlates most with the class label.\n",
    "\n",
    "- A value closer to 0 implies weaker correlation (exact 0 implying no correlation)\n",
    "- A value closer to 1 implies stronger positive correlation\n",
    "- A value closer to -1 implies stronger negative correlation\n",
    "\n",
    "Using the method _corr()_ gives us the Pearson correlation matrix between the variables that we pass to the method.\n",
    "\n",
    "We can then use seaborn to plot rectangular data as a color-encoded matrix.\n",
    "The parameter data needs to be a rectangular dataset of pairwise correlations.\n",
    "\n",
    "The resulting plot is clearly symmetrical: the Pearson Correlation Coefficient of A and B is the same of that of B and A."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,10))\n",
    "\n",
    "cor = data_dropped[['age', 'trestbps', 'chol', 'thalach', 'oldpeak', 'class']].corr()\n",
    "\n",
    "sns.heatmap(data=cor, annot=True, cmap=plt.cm.Reds);\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pairplot\n",
    "\n",
    "Pairplots are used to investigate pairwise relationships.\n",
    "\n",
    "To use it, we make a list of the numerical values, and in hue we can pass the class, to give different colours to the samples from the different classes.\n",
    "\n",
    "On the diagonal we see the distribution of the different numerical variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "sns.pairplot(data_dropped[['age', 'trestbps', 'chol', 'thalach', 'oldpeak', 'class']], hue='class', height=2.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Outline:**\n",
    "\n",
    "1. Reading the file, Summary Statistics âœ…\n",
    "\n",
    "2. Missing Values and Imputation âœ…\n",
    "\n",
    "3. EDA (Plotting, Outliers, Highly correlated features) âœ…\n",
    "\n",
    "4. Standardization ðŸ”œ\n",
    "\n",
    "5. Principal Component Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Standardization \n",
    "\n",
    "Standardization is a crucial step before performing PCA, since we are interested in the components that maximize the variance.\n",
    "We only standardize the numerical features, not the categorical ones, even if they are represented with numbers. Class should not be standardized either!\n",
    "\n",
    "The standardized value $z$ is calculated by removing the mean and subtracting the mean and dividing by the _standard_ deviation, from which it gets its name.\n",
    "The formula is:\n",
    "\n",
    "$$z = \\frac{x - \\mu}{\\sigma}$$\n",
    "\n",
    "where x is each of the values for a feature, $\\mu$ is the average value of all the values for that feature, and $\\sigma$ is the standard deviation for that feature.\n",
    "We repeat the process for all the **numerical features**.\n",
    "\n",
    "Standardized features have a mean of 0 and a standard deviation of 1, due to how $z$ is calculated.\n",
    "\n",
    "It is also called Z-Normalization, Z-Score, or Z-Score Normalization, so don't get scared if you hear a different name, you already know the technique!\n",
    "\n",
    "There is at least another similar procedure, called Normalization.\n",
    "Normalization is also only applied to the numerical features, and it should never be applied to the class label.\n",
    "\n",
    "The normalized value $x_{norm}$ is calculated by subtracting the minimum value $x_{min}$ to each value in a feature, and then dividing by the difference between the maximum and the minimum value $x_{max}$ and $x_{min}$.\n",
    "The formula is:\n",
    "\n",
    "$$x_{norm} = \\frac{(x - x_{min})}{(x_{max} - x_{min})} $$\n",
    "\n",
    "The normalized value has a minimum of 0 and a maximum of 1.\n",
    "\n",
    "Here is a small table summarizing the differences:\n",
    "\n",
    "|                  |                                                                       Standardization                                                                       |                                                                                                    Normalization                                                                                                    |\n",
    "|:----------------:|:-----------------------------------------------------------------------------------------------------------------------------------------------------------:|:-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------:|\n",
    "|      Formula     |                                                                     $z = \\frac{x-u}{s}$                                                                     |                                                                                   $x_{norm}=\\frac{(x-x_{min})}{(x_{max}-x_{min})}$                                                                                  |\n",
    "|   Distribution   |                                                                         Mean=0, SD=1                                                                        |                                                                                                       Unknown                                                                                                       |\n",
    "|       Range      |                                                                           Unknown                                                                           |                                                                                                       $[0,1]$                                                                                                       |\n",
    "| Why do we use it | We are interested in knowing how much something differs from the average: A standardized value of 1 tells us that the original value is 1 SD over the mean. | We want to compare values measured using different units. This reduces the influence created by using smaller units, like mm, which would otherwise appear greated than that created by using bigger units, like m. |\n",
    "\n",
    "In this notebook we will apply Standardization, since we have measures that are not only taken using different units, but also representing completely different concepts, so it makes more sense to check their difference from the average, rather than trying to bring, for example, age and cholesterol to the same unit.\n",
    "\n",
    "We will do it using StandardScaler, from sklearn. Normalization can be applied using MinMaxScaler, still from sklearn.\n",
    "\n",
    "Information about StandardScaler: [link](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html)\n",
    "\n",
    "Important methods:\n",
    "- _fit()_ computes the mean and std to be used for later scaling.\n",
    "- _transform()_ uses a previously computed mean and std to autoscale the data\n",
    "- *fit_transform()*  does both at the same time. So you can do it with 1 line of code instead of 2.\n",
    "\n",
    "\n",
    "Compare the effects of different scalers from sklearn: [link](https://scikit-learn.org/stable/auto_examples/preprocessing/plot_all_scaling.html#sphx-glr-auto-examples-preprocessing-plot-all-scaling-py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "numerical = [\"age\", \"trestbps\", \"chol\", \"thalach\", \"oldpeak\"] #list of numerical features\n",
    "\n",
    "X = data_dropped[numerical]\n",
    "\n",
    "scaler = StandardScaler().fit(X)\n",
    "\n",
    "data_scaled = scaler.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_scaled = pd.DataFrame(data_scaled, columns=numerical)\n",
    "data_scaled.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BONUS: Standardization before and after train/test splitting\n",
    "We can now use a simple example table to check why it is important to standardize things at the right time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now create a table with the median values of the columns of the original dataset, the training set and the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dataframe with two columns: patient number and cholesterol. Three patients should have 500, 498 and 502 cholesterol levels, respectively, and two patients should have 100 and 102 cholesterol levels, respectively.\n",
    "complete_data = pd.DataFrame({'patient_number': [1, 2, 3, 4, 5], 'cholesterol': [500, 498, 502, 100, 102]})\n",
    "train_data = complete_data.iloc[:3]\n",
    "test_data = complete_data.iloc[3:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler2 = StandardScaler().fit(train_data[['cholesterol']])\n",
    "train_data_scaled = scaler2.transform(train_data[['cholesterol']])\n",
    "test_data_scaled = scaler2.transform(test_data[['cholesterol']])\n",
    "\n",
    "scaler3 = StandardScaler().fit(complete_data[['cholesterol']])\n",
    "complete_data_scaled = scaler3.transform(complete_data[['cholesterol']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('train_data_scaled: \\n', train_data_scaled)\n",
    "print('test_data_scaled: \\n', test_data_scaled)\n",
    "print('complete_data_scaled: \\n', complete_data_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Outline:**\n",
    "\n",
    "1. Reading the file, Summary Statistics âœ…\n",
    "\n",
    "2. Missing Values and Imputation âœ…\n",
    "\n",
    "3. EDA (Plotting, Outliers, Highly correlated features) âœ…\n",
    "\n",
    "4. Standardization âœ…\n",
    "\n",
    "5. Principal Component Analysis ðŸ”œ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Principal Component Analysis (PCA)\n",
    "\n",
    "PCA creates new variables, uncorrelated (orthogonal), that are a linear combination of the original variables.\n",
    "The new variables will be in order of importance, ordered according to how much of the original variance they explain:\n",
    "- new variable 1 will explain as much of the variance as possible\n",
    "- new variable 2 will explain as much of the remaining variance as possible\n",
    "- new variable 3 will explain as much of the remaining variance as possible\n",
    "- ...\n",
    "\n",
    "PCA also helps you visualize your data: it's easier to visualize a two-dimensional plot than a 100-dimensional one.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import PCA\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will not initialize a PCA object, specifying the number of desired components.\n",
    "We can then:\n",
    "- use the _fit()_ method on our dataset\n",
    "- use the _transform()_ method to transform our dataset\n",
    "\n",
    "or\n",
    "- just use the *fit_transform()* method on our dataset, to do both all in one line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)\n",
    "x_pca = pca.fit_transform(data_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's important to create a new variable when transforming a dataframe with PCA, otherwise we would lose the original data.\n",
    "\n",
    "Once the PCA object is fitted, we can check how much of the original variance is explained by each of our Principal Components, using the *explained_variance_ratio_* attribute.\n",
    "\n",
    "Note that, even if you change the number of Principal Components, the amount of variance explained by each of the first two will not change: the first component explains 35% and the second 21% of the original variance.\n",
    "\n",
    "Even changing their number, the components are always the same, we just include more or less of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the explained variance ratio\n",
    "print(pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now check the shapes of the original dataframe and of the new object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#original shape\n",
    "data_scaled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#shape after PCA\n",
    "x_pca.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the principal components with colors on the class\n",
    "\n",
    "Since the x_pca variable contains only two components, we can use it to plot our data.\n",
    "\n",
    "There are some advantages in doing so, like being able to see if there is a clear separation between the classes.\n",
    "Real world datasets, like this one, often don't have such a separation, but toy datasets might have it.\n",
    "\n",
    "A disadvantage are that it's not always immediate to interpret the components: component one is not a single feature from the original dataset, like age or cholesterol, but a linear combination of all the numerical features instead.\n",
    "\n",
    "We can now plot a scatterplot of the Principal Components that we calculated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,6))\n",
    "plt.scatter(x=x_pca[:,0], y=x_pca[:,1], c=data_dropped['class'], cmap='rainbow')\n",
    "plt.xlabel('1st PC')\n",
    "plt.ylabel('2nd PC')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA components\n",
    "\n",
    "When printing the *components_* attribute we can see an array, in which each row represents a principal component, and each column relates back to the original features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_comp = pd.DataFrame(pca.components_,  columns=numerical)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_comp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Heatmap for components\n",
    "\n",
    "By printing a heatmap of *df_comp* we can see the correlation between the features and the components, to determine which features are more important for which components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,6))\n",
    "sns.heatmap(df_comp, cmap='plasma')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Outline:**\n",
    "\n",
    "1. Reading the file, Summary Statistics âœ…\n",
    "\n",
    "2. Missing Values and Imputation âœ…\n",
    "\n",
    "3. EDA (Plotting, Outliers, Highly correlated features) âœ…\n",
    "\n",
    "4. Standardization âœ…\n",
    "\n",
    "5. Principal Component Analysis âœ…"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# END OF LAB 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See you in Homework1!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4a3821e50dfd29f54393a38062d93a54d0c9d954cd67861638d013f261604981"
  },
  "kernelspec": {
   "display_name": "Python 3.7.4 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
